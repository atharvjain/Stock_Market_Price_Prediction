{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJaZRJN-V4yG",
        "outputId": "3f6d5ab4-6a01-477a-cf2a-19e1a37d8f6b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.39)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports, Data Loading & Processing"
      ],
      "metadata": {
        "id": "84oI7Cwl3zps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s5_pZGGeUQdK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import optuna\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"train.csv\", index_col=0)\n",
        "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
        "# Convert to time-series friendly format\n",
        "train_df = train_df.T\n",
        "train_df.index.name = \"Date\"\n",
        "train_df.reset_index(inplace=True)\n",
        "# Convert the Date column to actual datetime objects\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'], format='%d/%m/%Y')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding Moving Avg Feature Engineering\n",
        "\n",
        "Computing Moving Average and adding it as a new feature. Using moving avg to provide smoother trends since raw stock prices can be noisy.\n",
        "\n",
        "Creating ma7 for weekly window, ma14 for 14 days window and so on....\n",
        "Each window captures trends at different timescale. And handling Nan values for early days in dataset.\n",
        "\n",
        "Scaling the dataset to converge faster."
      ],
      "metadata": {
        "id": "VGJsixAx4UPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_moving_averages(df):\n",
        "    \"\"\"Add moving average features to the dataframe.\"\"\"\n",
        "    # Storing Date column copy and removing it for calculation\n",
        "    date_col = df['Date'].copy()\n",
        "    df_numeric = df.drop('Date', axis=1)\n",
        "\n",
        "    # Calculate moving averages for 7, 14, 30, and 60-day windows\n",
        "    ma7 = df_numeric.rolling(window=7).mean()\n",
        "    ma14 = df_numeric.rolling(window=14).mean()\n",
        "    ma30 = df_numeric.rolling(window=30).mean()\n",
        "    ma60 = df_numeric.rolling(window=60).mean()\n",
        "\n",
        "    # Rename columns of the moving average data to reflect the window size\n",
        "    ma7.columns = [f\"{col}_ma7\" for col in ma7.columns]\n",
        "    ma14.columns = [f\"{col}_ma14\" for col in ma14.columns]\n",
        "    ma30.columns = [f\"{col}_ma30\" for col in ma30.columns]\n",
        "    ma60.columns = [f\"{col}_ma60\" for col in ma60.columns]\n",
        "    #Adding moving avg to orignal data\n",
        "    result = pd.concat([df_numeric, ma7, ma14, ma30, ma60], axis=1)\n",
        "\n",
        "    # Filling missing values (NaNs) for the first few rows\n",
        "    result.insert(0, 'Date', date_col)\n",
        "    for company in df_numeric.columns:\n",
        "        mask7 = result[f\"{company}_ma7\"].isna()\n",
        "        result.loc[mask7, f\"{company}_ma7\"] = result.loc[mask7, company]\n",
        "\n",
        "        mask14 = result[f\"{company}_ma14\"].isna()\n",
        "        result.loc[mask14, f\"{company}_ma14\"] = result.loc[mask14, company]\n",
        "\n",
        "        mask30 = result[f\"{company}_ma30\"].isna()\n",
        "        result.loc[mask30, f\"{company}_ma30\"] = result.loc[mask30, company]\n",
        "\n",
        "        mask60 = result[f\"{company}_ma60\"].isna()\n",
        "        result.loc[mask60, f\"{company}_ma60\"] = result.loc[mask60, company]\n",
        "    return result\n",
        "\n",
        "# Apply moving average feature engineering\n",
        "enhanced_df = add_moving_averages(train_df)\n",
        "print(f\"Original DataFrame shape: {train_df.shape}\")\n",
        "print(f\"Enhanced DataFrame shape: {enhanced_df.shape}\")\n",
        "\n",
        "enhanced_df.set_index('Date', inplace=True)\n",
        "\n",
        "# Use StandardScaler to normalize all features to have mean 0 and standard deviation 1\n",
        "scaler = StandardScaler()\n",
        "df_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(enhanced_df),\n",
        "    columns=enhanced_df.columns,\n",
        "    index=enhanced_df.index\n",
        ")\n",
        "\n",
        "print(f\"Scaled DataFrame shape: {df_scaled.shape}\")\n",
        "print(f\"Number of features: {df_scaled.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4YlqM__20Z8",
        "outputId": "91e143e6-6600-46bf-b36a-8b98be184b11"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame shape: (3021, 443)\n",
            "Enhanced DataFrame shape: (3021, 2211)\n",
            "Scaled DataFrame shape: (3021, 2210)\n",
            "Number of features: 2210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Time-Series Sequence for Model Input\n",
        "\n",
        "Transforming the scaled dataset into fixed-length sequences using a 30-day window.\n",
        "\n",
        "And spliting the data into training and validation sets for model learning."
      ],
      "metadata": {
        "id": "hsPnl__j5ALx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 30\n",
        "# Create sequences function\n",
        "def create_sequences(data, window_size=30):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(data) - window_size):\n",
        "        X.append(data[i:i + window_size])\n",
        "        y.append(data[i + window_size])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(df_scaled.values, window_size)\n",
        "\n",
        "# Spliting data into (80/20 split)\n",
        "split_index = int(0.8 * len(X))\n",
        "X_train, X_val = X[:split_index], X[split_index:]\n",
        "y_train, y_val = y[:split_index], y[split_index:]\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x75RzlC3VUCE",
        "outputId": "214fb233-f972-4feb-f5ed-20f575a41d83"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: torch.Size([2392, 30, 2210]), y_train shape: torch.Size([2392, 2210])\n",
            "X_val shape: torch.Size([599, 30, 2210]), y_val shape: torch.Size([599, 2210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Dataset and DataLoaders\n",
        "Defining a custom PyTorch dataset class StockDataset to handle input features and targets.\n",
        "\n",
        "Later wrapping the training and validation data in DataLoader object, for efficient mini-batch use during training."
      ],
      "metadata": {
        "id": "GFQWZNGq7-b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a custom dataset class for stock prediction\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "\n",
        "# Creating the training dataset using training features and labels\n",
        "train_dataset = StockDataset(X_train, y_train)\n",
        "\n",
        "# Creating the validation dataset using validation features and labels\n",
        "val_dataset = StockDataset(X_val, y_val)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# Previewing the shape of one batch of data from the training loader to confirm data is compatible\n",
        "for features, targets in train_loader:\n",
        "    print(f\"Batch features shape: {features.shape}, targets shape: {targets.shape}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeLqzI5DVX-G",
        "outputId": "f298b032-bbc7-4c49-d4f3-7f2f4b801c18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch features shape: torch.Size([32, 30, 2210]), targets shape: torch.Size([32, 2210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining LSTM Model\n",
        "\n",
        "Defining an LSTMModel in PyTorch with two stacked LSTM layers (128 and 64 hidden units), using dropout (0.2) for regularization.\n",
        "\n",
        "The model processes 30-day input sequences and predicts the next day's stock values using a fully connected layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "HSJshKYEAk9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1=128, hidden_dim2=64, dropout=0.2, output_dim=None):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        # First LSTM layer: Takes input features and outputs hidden states of size hidden_dim1\n",
        "        # Dropout layer to reduce overfitting after the first LSTM\n",
        "        self.lstm1 = nn.LSTM(input_dim, hidden_dim1, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        # Second LSTM layer: Takes the output from the first LSTM and processes it further\n",
        "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        if output_dim is None:\n",
        "            output_dim = y_train.shape[1]  # Number of companies\n",
        "\n",
        "        # Fully connected layer to map LSTM output to final predictions\n",
        "        self.fc = nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through the first LSTM layer\n",
        "        out, _ = self.lstm1(x)\n",
        "        x = self.dropout1(out) # Pass the output to dropout\n",
        "\n",
        "        # Pass through the second LSTM layer\n",
        "        out, _ = self.lstm2(x)\n",
        "        x = self.dropout2(out) # Pass the output to dropout\n",
        "\n",
        "        # Take the last output and pass through fully connected layer\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return x"
      ],
      "metadata": {
        "id": "O_ruLpWBXv9q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "DO2WKnvMDl_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameter Optimization with Optuna\n",
        "\n",
        "Defining an Optuna objective function to optimize hyperparameters of an LSTMModel by minimizing validation loss.\n",
        "\n",
        "Optuna tests different combinations of hidden layer sizes, dropout rate, and learning rate across 30 trials.\n",
        "\n",
        "For each trial, it builds the model, trains it for 5 epochs using MSEloss, and evaluates performance on the validation set."
      ],
      "metadata": {
        "id": "nl1a9pBUFXW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Optuna objective function\n",
        "def objective(trial):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Hyperparameters to tune\n",
        "    hidden_dim1 = trial.suggest_int('hidden_dim1', 64, 256) # Number of hidden units in the first LSTM layer\n",
        "    hidden_dim2 = trial.suggest_int('hidden_dim2', 32, 128) # Number of hidden units in the second LSTM layer\n",
        "    dropout = trial.suggest_float('dropout', 0.1, 0.5) # Dropout rate between layers\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True) # Learning rate\n",
        "\n",
        "    # Build model with current hyperparameters\n",
        "    model = LSTMModel(\n",
        "        input_dim=X_train.shape[2],  # Number of features\n",
        "        hidden_dim1=hidden_dim1,\n",
        "        hidden_dim2=hidden_dim2,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    # Defining Loss and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train model for epochs\n",
        "    model.train()\n",
        "    for epoch in range(5):  # Using fewer epochs for hyperparameter search\n",
        "        for features, targets in train_loader:\n",
        "            features, targets = features.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass: get predictions from the model\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass: compute gradients\n",
        "            optimizer.zero_grad() # Clear the previous gradients\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in val_loader:\n",
        "            features, targets = features.to(device), targets.to(device)\n",
        "            outputs = model(features)\n",
        "            val_loss += criterion(outputs, targets).item() # Accumulate validation loss\n",
        "\n",
        "    val_loss /= len(val_loader)  # Averaging validation loss across all batches\n",
        "    return val_loss\n",
        "\n",
        "# Running Optuna study to find the best hyperparameters\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=30)\n",
        "print(\"Best hyperparameters:\", study.best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv1RtlV-ZAoS",
        "outputId": "0ad8941c-c8f9-41af-d0ae-de8191c77959"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-27 15:59:00,545] A new study created in memory with name: no-name-2c7575ab-b9ad-4a04-bd15-e56f3af9a12a\n",
            "[I 2025-03-27 15:59:22,570] Trial 0 finished with value: 1.3677880685580404 and parameters: {'hidden_dim1': 206, 'hidden_dim2': 122, 'dropout': 0.45085887540821323, 'learning_rate': 2.5511519221885295e-05}. Best is trial 0 with value: 1.3677880685580404.\n",
            "[I 2025-03-27 15:59:29,405] Trial 1 finished with value: 1.3842658369164718 and parameters: {'hidden_dim1': 171, 'hidden_dim2': 69, 'dropout': 0.31170321380093285, 'learning_rate': 2.9810593752292393e-05}. Best is trial 0 with value: 1.3677880685580404.\n",
            "[I 2025-03-27 15:59:34,913] Trial 2 finished with value: 1.3014904621400332 and parameters: {'hidden_dim1': 144, 'hidden_dim2': 36, 'dropout': 0.32648710860554675, 'learning_rate': 0.0005639394048912729}. Best is trial 2 with value: 1.3014904621400332.\n",
            "[I 2025-03-27 15:59:41,735] Trial 3 finished with value: 1.3257429615447396 and parameters: {'hidden_dim1': 180, 'hidden_dim2': 85, 'dropout': 0.4081743575310388, 'learning_rate': 0.002386736120655785}. Best is trial 2 with value: 1.3014904621400332.\n",
            "[I 2025-03-27 15:59:47,612] Trial 4 finished with value: 1.298761574845565 and parameters: {'hidden_dim1': 227, 'hidden_dim2': 86, 'dropout': 0.4716296321479422, 'learning_rate': 0.0008365768005218663}. Best is trial 4 with value: 1.298761574845565.\n",
            "[I 2025-03-27 15:59:52,364] Trial 5 finished with value: 1.2947187768785577 and parameters: {'hidden_dim1': 94, 'hidden_dim2': 84, 'dropout': 0.13097820171849409, 'learning_rate': 0.0007187989690229314}. Best is trial 5 with value: 1.2947187768785577.\n",
            "[I 2025-03-27 15:59:58,456] Trial 6 finished with value: 1.3263830495508093 and parameters: {'hidden_dim1': 236, 'hidden_dim2': 121, 'dropout': 0.2433470055800347, 'learning_rate': 0.00306418014513102}. Best is trial 5 with value: 1.2947187768785577.\n",
            "[I 2025-03-27 16:00:03,597] Trial 7 finished with value: 1.3473148832195683 and parameters: {'hidden_dim1': 135, 'hidden_dim2': 101, 'dropout': 0.26305253475300994, 'learning_rate': 0.00011247797141415776}. Best is trial 5 with value: 1.2947187768785577.\n",
            "[I 2025-03-27 16:00:09,920] Trial 8 finished with value: 1.2893376868022115 and parameters: {'hidden_dim1': 211, 'hidden_dim2': 85, 'dropout': 0.168478052365363, 'learning_rate': 0.0006009227007955891}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:00:16,303] Trial 9 finished with value: 1.5147438817902614 and parameters: {'hidden_dim1': 226, 'hidden_dim2': 37, 'dropout': 0.28957210056208893, 'learning_rate': 1.0036976914977577e-05}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:00:22,426] Trial 10 finished with value: 1.3263847874967676 and parameters: {'hidden_dim1': 254, 'hidden_dim2': 59, 'dropout': 0.10778443791873976, 'learning_rate': 0.00013030560638386545}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:00:27,211] Trial 11 finished with value: 1.3808425492361973 and parameters: {'hidden_dim1': 81, 'hidden_dim2': 106, 'dropout': 0.1055644027967321, 'learning_rate': 0.006118671404018178}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:00:31,931] Trial 12 finished with value: 1.3237504284632833 and parameters: {'hidden_dim1': 90, 'hidden_dim2': 71, 'dropout': 0.18221565506832957, 'learning_rate': 0.00026099307933185917}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:00:37,163] Trial 13 finished with value: 1.3093520575448085 and parameters: {'hidden_dim1': 104, 'hidden_dim2': 97, 'dropout': 0.18770036874097146, 'learning_rate': 0.0011848817895141178}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:00:42,073] Trial 14 finished with value: 1.36070558742473 and parameters: {'hidden_dim1': 116, 'hidden_dim2': 54, 'dropout': 0.17358777059891112, 'learning_rate': 0.0003032011486761976}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:00:48,305] Trial 15 finished with value: 1.3363420539780666 and parameters: {'hidden_dim1': 194, 'hidden_dim2': 79, 'dropout': 0.14473790835485234, 'learning_rate': 0.0016437396405680153}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:00:53,499] Trial 16 finished with value: 1.3535514379802502 and parameters: {'hidden_dim1': 149, 'hidden_dim2': 92, 'dropout': 0.21148486294405672, 'learning_rate': 0.009290560482102458}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:00:58,906] Trial 17 finished with value: 1.3098876664513035 and parameters: {'hidden_dim1': 73, 'hidden_dim2': 111, 'dropout': 0.3750595955832998, 'learning_rate': 0.0005285922206420535}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:01:03,806] Trial 18 finished with value: 1.331546891676752 and parameters: {'hidden_dim1': 124, 'hidden_dim2': 72, 'dropout': 0.14636766329814244, 'learning_rate': 9.523901112894287e-05}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:01:09,399] Trial 19 finished with value: 1.3359226804030568 and parameters: {'hidden_dim1': 202, 'hidden_dim2': 62, 'dropout': 0.23068514430732684, 'learning_rate': 0.004374481354644403}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:01:15,003] Trial 20 finished with value: 1.3447660709682263 and parameters: {'hidden_dim1': 161, 'hidden_dim2': 48, 'dropout': 0.15354620987979103, 'learning_rate': 0.00019223640343248586}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:01:23,656] Trial 21 finished with value: 1.305260353966763 and parameters: {'hidden_dim1': 222, 'hidden_dim2': 88, 'dropout': 0.4981242671306054, 'learning_rate': 0.0008562744633697767}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:01:30,550] Trial 22 finished with value: 1.3054368386143131 and parameters: {'hidden_dim1': 256, 'hidden_dim2': 80, 'dropout': 0.3505083025400742, 'learning_rate': 0.0005167846536645691}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:01:36,981] Trial 23 finished with value: 1.3190546443587856 and parameters: {'hidden_dim1': 183, 'hidden_dim2': 94, 'dropout': 0.4871038252905636, 'learning_rate': 0.0008282076936269822}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:01:42,472] Trial 24 finished with value: 1.3346582604082007 and parameters: {'hidden_dim1': 214, 'hidden_dim2': 79, 'dropout': 0.384862846384433, 'learning_rate': 0.0015873487881433886}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:01:48,486] Trial 25 finished with value: 1.2943091643484015 and parameters: {'hidden_dim1': 238, 'hidden_dim2': 111, 'dropout': 0.27968299719905115, 'learning_rate': 0.0004222191677779849}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:01:54,278] Trial 26 finished with value: 1.322759813384006 and parameters: {'hidden_dim1': 242, 'hidden_dim2': 114, 'dropout': 0.26257213181526684, 'learning_rate': 6.148720866708131e-05}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:01:59,464] Trial 27 finished with value: 1.3366161459370662 and parameters: {'hidden_dim1': 65, 'hidden_dim2': 102, 'dropout': 0.13282184511329745, 'learning_rate': 0.00031896189856692956}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:02:04,426] Trial 28 finished with value: 1.3305145784428245 and parameters: {'hidden_dim1': 99, 'hidden_dim2': 108, 'dropout': 0.21312578231776952, 'learning_rate': 0.00040934065833799806}. Best is trial 8 with value: 1.2893376868022115.\n",
            "[I 2025-03-27 16:02:09,857] Trial 29 finished with value: 1.3154287291200537 and parameters: {'hidden_dim1': 205, 'hidden_dim2': 126, 'dropout': 0.4180104931117934, 'learning_rate': 0.00017624394053041557}. Best is trial 8 with value: 1.2893376868022115.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'hidden_dim1': 211, 'hidden_dim2': 85, 'dropout': 0.168478052365363, 'learning_rate': 0.0006009227007955891}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM Model with best Hyperparameters\n",
        "\n",
        "Training the final LSTMModel using the best hyperparameters found by Optuna.\n",
        "\n",
        "Running for 30 epochs, optimizing with the Adam optimizer and evaluating performance on the validation set after each epoch to monitor training progress."
      ],
      "metadata": {
        "id": "0lXZj0kiHrt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train final model with best hyperparameters\n",
        "best_params = study.best_params\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "final_model = LSTMModel(\n",
        "    input_dim=X_train.shape[2],\n",
        "    hidden_dim1=best_params['hidden_dim1'], # Best number of hidden units in the first LSTM layer\n",
        "    hidden_dim2=best_params['hidden_dim2'], # Best number of hidden units in the second LSTM layer\n",
        "    dropout=best_params['dropout'] # Best dropout rate\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Using the Adam optimizer with the best learning rate\n",
        "optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['learning_rate'])\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    final_model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    # Loop through all batches in the training data\n",
        "    for features, targets in train_loader:\n",
        "        features, targets = features.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = final_model(features)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Print training stats\n",
        "    train_loss /= len(train_loader) # Calculate the average training loss over all batches\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Validation after each epoch\n",
        "    final_model.eval()\n",
        "    val_loss = 0 # Reset validation loss\n",
        "    with torch.no_grad():\n",
        "        for features, targets in val_loader:\n",
        "            features, targets = features.to(device), targets.to(device)\n",
        "            outputs = final_model(features)\n",
        "            val_loss += criterion(outputs, targets).item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "scqIwdFfXrGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc126c57-7d65-4e4a-b651-2d4bc76716e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 0.6530\n",
            "Validation Loss: 1.3436\n",
            "Epoch 2/30, Loss: 0.5889\n",
            "Validation Loss: 1.2928\n",
            "Epoch 3/30, Loss: 0.5648\n",
            "Validation Loss: 1.2953\n",
            "Epoch 4/30, Loss: 0.5500\n",
            "Validation Loss: 1.2859\n",
            "Epoch 5/30, Loss: 0.5323\n",
            "Validation Loss: 1.2720\n",
            "Epoch 6/30, Loss: 0.5183\n",
            "Validation Loss: 1.2600\n",
            "Epoch 7/30, Loss: 0.5026\n",
            "Validation Loss: 1.2884\n",
            "Epoch 8/30, Loss: 0.4894\n",
            "Validation Loss: 1.3120\n",
            "Epoch 9/30, Loss: 0.4747\n",
            "Validation Loss: 1.3066\n",
            "Epoch 10/30, Loss: 0.4628\n",
            "Validation Loss: 1.2878\n",
            "Epoch 11/30, Loss: 0.4544\n",
            "Validation Loss: 1.2824\n",
            "Epoch 12/30, Loss: 0.4437\n",
            "Validation Loss: 1.2963\n",
            "Epoch 13/30, Loss: 0.4336\n",
            "Validation Loss: 1.2917\n",
            "Epoch 14/30, Loss: 0.4257\n",
            "Validation Loss: 1.2746\n",
            "Epoch 15/30, Loss: 0.4187\n",
            "Validation Loss: 1.3208\n",
            "Epoch 16/30, Loss: 0.4097\n",
            "Validation Loss: 1.3064\n",
            "Epoch 17/30, Loss: 0.4016\n",
            "Validation Loss: 1.3136\n",
            "Epoch 18/30, Loss: 0.3946\n",
            "Validation Loss: 1.2812\n",
            "Epoch 19/30, Loss: 0.3907\n",
            "Validation Loss: 1.3376\n",
            "Epoch 20/30, Loss: 0.3873\n",
            "Validation Loss: 1.2972\n",
            "Epoch 21/30, Loss: 0.3817\n",
            "Validation Loss: 1.3203\n",
            "Epoch 22/30, Loss: 0.3774\n",
            "Validation Loss: 1.3153\n",
            "Epoch 23/30, Loss: 0.3737\n",
            "Validation Loss: 1.3224\n",
            "Epoch 24/30, Loss: 0.3718\n",
            "Validation Loss: 1.3389\n",
            "Epoch 25/30, Loss: 0.3662\n",
            "Validation Loss: 1.3069\n",
            "Epoch 26/30, Loss: 0.3643\n",
            "Validation Loss: 1.3200\n",
            "Epoch 27/30, Loss: 0.3638\n",
            "Validation Loss: 1.3252\n",
            "Epoch 28/30, Loss: 0.3567\n",
            "Validation Loss: 1.3411\n",
            "Epoch 29/30, Loss: 0.3555\n",
            "Validation Loss: 1.3285\n",
            "Epoch 30/30, Loss: 0.3508\n",
            "Validation Loss: 1.3316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Predictions\n",
        "\n",
        "Using the trained LSTM model to make a prediction for the next day based on the most recent 30 days of data.\n",
        "\n",
        "The predicted values are inverse-transformed back to their original scale."
      ],
      "metadata": {
        "id": "zE8uJTO5I6Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    # Get the last sequence from the training data\n",
        "    last_sequence = df_scaled.values[-window_size:]\n",
        "    last_sequence = torch.tensor(last_sequence).unsqueeze(0).float().to(device)  # Add batch dimension\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = final_model(last_sequence).cpu().numpy()[0]\n",
        "    print(f\"Raw prediction shape: {prediction.shape}\")\n",
        "\n",
        "    # If prediction size matches df_scaled width, it includes all features\n",
        "    if len(prediction) == df_scaled.shape[1]:\n",
        "        # Extracting only the first 442 values which correspond to the original companies\n",
        "        prediction = prediction[:442]\n",
        "    elif len(prediction) != 442:\n",
        "        raise ValueError(f\"Unexpected prediction shape: {prediction.shape}. Expected either 442 or {df_scaled.shape[1]}\")\n",
        "\n",
        "    # Create a dummy array with all the original features plus enhanced ones\n",
        "    dummy = np.zeros((1, df_scaled.shape[1]))\n",
        "    # Place the company predictions in the first 442 columns\n",
        "    dummy[0, :442] = prediction\n",
        "\n",
        "    # Inverse transform the entire array\n",
        "    original_scale_data = scaler.inverse_transform(dummy)\n",
        "\n",
        "    # Extracting only the original company values\n",
        "    prediction_original_scale = original_scale_data[0, :442]\n",
        "\n",
        "# Updated submission file\n",
        "sample_submission['value'] = prediction_original_scale\n",
        "sample_submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Prediction completed and saved to submission.csv\")\n",
        "print(sample_submission.head())"
      ],
      "metadata": {
        "id": "o1m--Y1cZeXm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be8623b-1643-4234-d097-e18ff7d9c62c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw prediction shape: (2210,)\n",
            "Prediction completed and saved to submission.csv\n",
            "          ID     value\n",
            "0  company_0  0.370681\n",
            "1  company_1  0.499594\n",
            "2  company_2  0.766118\n",
            "3  company_3  0.521492\n",
            "4  company_4  0.277550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fZT3cMfw9Qfu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QCry4HrotQys"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}